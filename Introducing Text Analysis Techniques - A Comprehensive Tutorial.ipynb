{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;font-weight:bold\">Introducing Text Analysis Techniques: A Comprehensive Tutorial</h2>\n",
    "\n",
    "In this tutorial, we cover a wide range of text analysis techniques, focusing on individual sentences. Here are the topics we explore:\n",
    "\n",
    "1. Removing whitespace: Clean the text by eliminating unnecessary spaces.\n",
    "2. Remove periods: Eliminate periods from the text to simplify the analysis.\n",
    "3. Capitalizer: Convert text to uppercase or capitalize certain words.\n",
    "4. Anonymize or obfuscate sensitive information: Protect sensitive data while maintaining data integrity.\n",
    "5. Parse HTML: Extract valuable content from HTML sources, such as web pages.\n",
    "6. Removing punctuation: Split text into tokens by removing punctuation marks.\n",
    "7. Stop word removal: Eliminate common words with little semantic value from the analysis.\n",
    "8. Stemming and lemmatization: Reduce words to their root forms for better analysis and grouping.\n",
    "9. Tagging Parts of Speech: Assign grammatical labels to words to understand their roles in sentences.\n",
    "\n",
    "Throughout the tutorial, we provide explanations and code examples using popular Python libraries like NLTK and BeautifulSoup. By the end, you will have a solid understanding of these text analysis techniques and be ready to apply them to your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing whitespace**: The function of the code bellow is to remove whitespace at the beginning and end of each string in the `text_data` list and store the cleaned strings in a new list called `strip_whitespace`. When iterating through each string in `text_data` using list comprehension and applying `strip( )`, whitespace at the beginning and end of each string is stripped. This results in a `strip_whitespace` list that contains the original strings without the necessary whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text.\n",
    "text_data = [\"     Interrobang. By Aishwarya Henriette\",\n",
    "             \"Parking And Going. By Karl Gautier     \",\n",
    "             \"     Today Is The night. By Jarek Prakash     \"]\n",
    "\n",
    "# Strip whitespaces.\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove periods**: In this code, we have a list comprehension that iterates over each string in the `strip_whitespace` list. For each string, the `replace()` method is used to remove dots (\".\") by replacing them with an empty string (\"\"), effectively deleting the dots from the strings. The resulting list, stored in the `remove_periods` variable, contains the strings with periods removed. Finally, the `remove_periods` code is used to display the modified list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove periods.\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Capitalizer**: In this code, we define a function called capitalizer that takes a string as input and returns the uppercase version of that string using the upper ( ) method. The function has type annotations specifying that the input is of type str and the output is also of type str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create function.\n",
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "# Apply function.\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anonymize or obfuscate sensitive information**: The function uses the re.sub() function, which performs a regular expression based substitution. The regular expression pattern [a-zA-Z] matches any uppercase or lowercase letter of the English alphabet. The re.sub() function replaces all occurrences of the matching pattern with the \"X\" character. Essentially, this function replaces all letters in the string with \"X\".\n",
    "\n",
    "In the context of text analysis, there may be cases where you need to work with effective data such as people's names, ID numbers, addresses, etc. Overwriting or anonymizing this data can be an important practice to protect the privacy of information.\n",
    "\n",
    "By applying substitution or anonymization techniques, such as replacing letters with \"X\" or removing keywords, you can preserve the structure and format of the original text, but hide specific information.\n",
    "\n",
    "However, it is essential to remember that text analysis performed with modified data can provide different or less accurate results, since the original information has been altered. Therefore, it is important to carefully consider which changes are applied and how they might affect the analysis or final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library.\n",
    "import re\n",
    "\n",
    "# Create function.\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "# Apply function.\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `re` library in Python is used to work with regular expressions (also known as regex). Regular expressions are string patterns that let you perform search, match, and manipulation operations on strings in a flexible and powerful way.\n",
    "\n",
    "The `re` library offers a variety of functions and methods for working with regular expressions, including:\n",
    "\n",
    "- `re.search()`: Searches for a pattern in a string and returns the first match found.\n",
    "- `re.match()`: Checks if the pattern matches the beginning of the string.\n",
    "- `re Return.findall()`: all occurrences of the pattern in a string as a list.\n",
    "- `re.sub()`: Replaces all occurrences of the pattern with other text in a string.\n",
    "- `re.split()`: Splits a string into a list of substrings based on the pattern.\n",
    "\n",
    "These are just some of the functions available in the `re` library. It provides a wide range of features for dealing with regular expressions, allowing you to perform sophisticated text manipulation tasks such as search, replace, validate, and request information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse HTML**: We use the BeautifulSoup library to parse the HTML code. We create a BeautifulSoup object soup by passing the HTML code and the lxml parser as arguments.\n",
    "\n",
    "Then we find the div element with the class \"full_name\" using the find() method. We specify the class name using the class_ parameter to avoid conflicts with the class keyword in Python. This returns a Tag object representing the div element.\n",
    "\n",
    "To extract the text inside the div element, we use the get_text() method with strip=True to remove any leading or trailing whitespace. The extracted text is stored in the full_name variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasegoAzra\n"
     ]
    }
   ],
   "source": [
    "# Load library.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create some HTML code.\n",
    "html = \"\"\"\n",
    "<div class='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\"\n",
    "\"\"\"\n",
    "\n",
    "# Parse HTML.\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# Find the div with the class \"full_name\" and extract the text.\n",
    "full_name_div = soup.find(\"div\", class_=\"full_name\")\n",
    "full_name = full_name_div.get_text(strip=True)\n",
    "\n",
    "# Show the extracted full name.\n",
    "print(full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"BeautifulSoup\" library (also known as BS4) is a Python library used to extract data from HTML and XML files. It provides a simple and intuitive way to parse and navigate HTML/XML documents, making it easy to get specific information from those documents.\n",
    "\n",
    "Some of the main features offered by the BeautifulSoup library are:\n",
    "\n",
    "1. Parsing HTML/XML documents: The library can parse HTML/XML documents and create a parse tree that represents the hierarchical structure of the document. This allows you to easily access specific elements in the document.\n",
    "\n",
    "\n",
    "2. Parse tree navigation: You can traverse the parse tree using methods like `find()` and `find_all()` to locate elements based on tags, classes, IDs and other attributes. This makes it easy to find and extract relevant information from the document.\n",
    "\n",
    "\n",
    "3. Data Extraction: The library provides simple methods to extract text, attributes and other data from HTML/XML elements. You can use these methods to get the content of tags, attribute values and relevant information within the document.\n",
    "\n",
    "\n",
    "4. Data manipulation: In addition to receiving data, BeautifulSoup also allows modifying and manipulating HTML/XML document elements. You can add, remove or change elements, attributes and text in documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing punctuation**: The code removes punctuation characters from the text using the translate( ) method and a dictionary of punctuation characters. It uses the unicodedata library to determine the punctuation category for each character. The resulting clean text is stored in the clean_text_data list.\n",
    "\n",
    "The purpose of this code is to demonstrate how to remove punctuation from text data using Unicode and the translate( ) method. Removing punctuation can be useful in text analysis tasks, such as sentiment analysis or natural language processing, where punctuation may not significantly contribute to the analysis and can be safely disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']\n"
     ]
    }
   ],
   "source": [
    "# Load libraries.\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# Create text.\n",
    "text_data = ['Hi!!!! I. Love. This. Song....', '10000% Agree!!!! #LoveIT', 'Right?!?!']\n",
    "\n",
    "# Create a dictionary of punctuation characters.\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# For each string, remove any punctuation characters.\n",
    "cleaned_text_data = [string.translate(punctuation) for string in text_data]\n",
    "\n",
    "# Show the cleaned text.\n",
    "print(cleaned_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicodedata library: This library provides a range of functions to work with Unicode characters. In this code, it is used to check the category of each character and determine if it falls under the \"P\" (punctuation) category.\n",
    "\n",
    "Sys library: This library provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. Here, it is used to get the value of sys.maxunicode, which represents the highest Unicode code point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of using `sys.maxunicode` in this code is to ensure that all possible punctuation characters are considered when creating the dictionary. It guarantees that the dictionary includes all relevant punctuation characters for the given Unicode encoding used by the Python interpreter, `sys.maxunicode` provides the highest Unicode code point available, and it is used in the code to ensure that the created `punctuation` dictionary covers all punctuation characters based on the specific Unicode encoding used by the Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLTK (Natural Language Toolkit) library is a widely used Python library for natural language processing. It provides a wide range of tools and resources to handle tasks related to natural language text processing.\n",
    "\n",
    "Here are some of the main functionalities and features offered by NLTK:\n",
    "\n",
    "1. Tokenization: NLTK provides several tokenization options, allowing you to break text into smaller units like words or phrases.\n",
    "\n",
    "2. Stop words: NLTK has a set of predefined stop words in different languages. These are common words such as \"a\", \"e\", \"o\", which are generally not very inducing for text analysis and can be removed.\n",
    "\n",
    "3. Stemming and lemmatization: These techniques allow you to reduce words to their root forms (stem) or to canonical forms (lemmas). This helps to deal with word variations, such as different verb or plural forms.\n",
    "\n",
    "4. Parsing and Parts of Speech Marking (POS): NLTK provides features to analyze the grammatical structure of sentences and assign labels to parts of speech (nouns, verbs, adjectives, etc.).\n",
    "\n",
    "5. Sentiment analysis: NLTK includes features for sentiment analysis, such as classifying text into sentiment categories (positive, negative, neutral) and dictionaries of words associated with sentiment polarities.\n",
    "\n",
    "6. Language models: NLTK provides a variety of language models trained on large volumes of text. These models can be used for tasks like predicting next words in a sequence or language detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/otawiochaves/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library.\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not necessary to run nltk.download('all') every time you use the NLTK (Natural Language Toolkit) library.\n",
    "\n",
    "This line of code is used to download all the resources available in the NLTK library, such as datasets, models, dictionaries, etc. However, downloading all of the resources can take time and unnecessarily take up disk space.\n",
    "\n",
    "Instead, we recommend downloading only the specific resources you need for your project. For example, if you are working with sentiment analysis, you can download the specific dataset related to that domain. You can use the following code to download a specific resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('resource_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 'resource_name' with the name of the specific resource you need to download. For example, if you need the sentiment polarity dataset, you can use nltk.download('vader_lexicon') to download that particular resource.\n",
    "\n",
    "That way, you control which resources are downloaded, save time and disk space, and avoid downloading resources you need for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**: Splitting the text into individual words or tokens. The purpose of the code below is to demonstrate how to use the word_tokenize function from NLTK to tokenize a string of text into individual words. Tokenization is a common preprocessing step in natural language processing tasks, as it allows for further analysis and manipulation of text on a word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create text.\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "# Tokenize words.\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop word removal**: Filtering out common words that do not contribute much to the overall meaning, such as \"and,\" \"the,\" or \"is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create word tokens.\n",
    "tokenized_words = ['i','am','going','to','go','to','the','store','and','park']\n",
    "\n",
    "# Load stop words.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Remove stop words.\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming and lemmatization**: Reducing words to their base or root form to consolidate similar variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create word tokens.\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "\n",
    "# Create stemmer.\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Apply stemmer.\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tagging Parts of Speech**: Tagging Parts of Speech, also known as Part-of-Speech (POS) Tagging, is the process of assigning Part-of-Speech labels to each word in a text. Each word is marked with a label that indicates its part of speech, such as noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries.\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Create text.\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# Use pre-trained part of speech tagger.\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# Show parts of speech.\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example you mentioned, `[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]`, is a representation of POS tagging applied to a sequence of words. Each element in the list is a pair, where the first element is the word and the second element is the assigned POS tag for that word.\n",
    "\n",
    "Here's the interpretation of each pair in the example:\n",
    "\n",
    "- `('Chris', 'NNP')`: In this case, the word \"Chris\" is assigned the tag `NNP`, which represents a singular proper noun.\n",
    "- `('loved', 'VBD')`: The word \"loved\" is assigned the tag `VBD`, indicating a past tense verb.\n",
    "- `('outdoor', 'RP')`: In this case, \"outdoor\" is tagged with the `RP` tag, which is an adverbial particle.\n",
    "- `('running', 'VBG')`: The word \"running\" is assigned the tag `VBG`, representing a verb in the gerund form.\n",
    "\n",
    "These tags are based on specific conventions or tag sets used to represent different grammatical classes in a text. The combination of words and their POS tags can be used in various natural language processing tasks, such as syntactic analysis, sentiment analysis, information extraction, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering words based on POS tags can be important in text analysis for several reasons:\n",
    "\n",
    "1. Keyword identification: By filtering only certain parts of speech, such as nouns, you can identify the most relevant keywords or terms in a text. This can be useful for extracting essential information, doing topical analysis, or summarizing content.\n",
    "\n",
    "\n",
    "2. Noise Reduction: Often certain parts of speech such as auxiliary verbs, pronouns or prepositions may not be relevant for text analysis in certain contexts. Filtering based on POS tags can help remove these secondary words and reduce noise in the data.\n",
    "\n",
    "\n",
    "3. Analysis: POS tags provide information about the grammatical role of each word in a sentence. This can be used for parsing, which involves understanding the grammatical structure of a sentence. By identifying the grammatical classes of words, it is possible to perform more advanced analysis, such as identifying subject, object, modifiers, among others.\n",
    "\n",
    "\n",
    "4. Sentiment classification: In some sentiment analysis applications, certain parts of speech, such as adjectives and nouns, are more relevant for determining the emotional polarity of a text. Filtering based on POS tags can help focus on the most impactful and informative words for sentiment ranking.\n",
    "\n",
    "\n",
    "In summary, word filtering based on POS tags allows for a refinement of text analysis, focusing on specific parts of speech that are relevant to the context and purpose of the analysis. This can lead to more accurate results and more relevant insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;font-weight:bold\">Reference list</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6. Handling Text, Machine Learning with Python Cookbook by Chris Albon Published by O'Reilly Media, Inc., 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
